{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python3.6\n",
    "import random as r\n",
    "import json\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from AbstractArchitecture import AbstractArchitecture\n",
    "from DenseEncoder import DenseEncoder\n",
    "from DenseDecoder import DenseDecoder\n",
    "from NormalizedMeanSquaredError import NormalizedMeanSquaredError as NMSE\n",
    "from plot_model_prediction import plot_model_prediction\n",
    "\n",
    "\n",
    "# Set Experiment Specifics\n",
    "expt_name = \"Experiment_00\"\n",
    "data_file_prefix = './data/NLSL_expt1'  ## FILL IN HERE (from file name)\n",
    "\n",
    "# Network architecture design\n",
    "l = 20  # Latent space size\n",
    "\n",
    "activation = \"relu\"\n",
    "initializer = tf.keras.initializers.VarianceScaling()\n",
    "reg_lambda_l2 = 1e-6\n",
    "regularizer = tf.keras.regularizers.l2(reg_lambda_l2)\n",
    "\n",
    "act_layer = dict(activation=activation,\n",
    "                 kernel_initializer=initializer,\n",
    "                 kernel_regularizer=regularizer)\n",
    "lin_layer = dict(activation=None,\n",
    "                 kernel_initializer=initializer,\n",
    "                 kernel_regularizer=regularizer)\n",
    "latent_config = dict(activation=None, \n",
    "                     kernel_regularizer=regularizer,\n",
    "                     use_bias=False)\n",
    "\n",
    "encoder_layers = 3\n",
    "decoder_layers = 4\n",
    "add_identity = True\n",
    "\n",
    "# Model training settings\n",
    "## Set optimizer\n",
    "optimizer = keras.optimizers.Adam\n",
    "optimizer_opts = {}\n",
    "\n",
    "# Callback function(s) and fit method options\n",
    "cbs = [keras.callbacks.EarlyStopping(patience=10)]\n",
    "\n",
    "# Batch size for model training\n",
    "batch_size = 64\n",
    "\n",
    "# Time to train autoencoders only and full models for initial seeding test\n",
    "aec_only_time = 1  # minutes\n",
    "full_model_time = 1  # minutes \n",
    "\n",
    "# This number is used to compute number of epochs for full-model training\n",
    "final_model_train_hrs = 0.1\n",
    "\n",
    "\n",
    "############################################\n",
    "### Everything below here is automated!! ###\n",
    "############################################\n",
    "\n",
    "# Step 0. Assign a random number generator seed\n",
    "x = r.randint(0, 10**(10))\n",
    "r.seed(x)\n",
    "\n",
    "# Step 1. Load in the data\n",
    "data_train_u = np.load(\"{}_train1_u.npy\".format(data_file_prefix))\n",
    "data_train_f = np.load(\"{}_train1_f.npy\".format(data_file_prefix))\n",
    "data_val_u = np.load(\"{}_val_u.npy\".format(data_file_prefix))\n",
    "data_val_f = np.load(\"{}_val_f.npy\".format(data_file_prefix))\n",
    "data_test_u1 = np.load(\"{}_test1_u.npy\".format(data_file_prefix))\n",
    "data_test_f1 = np.load(\"{}_test1_f.npy\".format(data_file_prefix))\n",
    "data_test_u = np.load(\"{}_test2_u.npy\".format(data_file_prefix))\n",
    "data_test_f = np.load(\"{}_test2_f.npy\".format(data_file_prefix))\n",
    "\n",
    "# Step 2. Set up the model architecture\n",
    "_, n = data_train_u.shape\n",
    "\n",
    "encoder_config = {'units_full': n,\n",
    "                  'num_layers': encoder_layers,\n",
    "                  'actlay_config': act_layer,\n",
    "                  'linlay_config': lin_layer,\n",
    "                  'add_init_fin': add_identity}\n",
    "\n",
    "decoder_config = {'units_full': n,\n",
    "                  'num_layers': decoder_layers,\n",
    "                  'actlay_config': act_layer,\n",
    "                  'linlay_config': lin_layer,\n",
    "                  'add_init_fin': add_identity}\n",
    "\n",
    "# Aggregate settings for model architecture\n",
    "architecture_config = {\"units_latent\": l,\n",
    "                       \"u_encoder_block\": DenseEncoder(**encoder_config),\n",
    "                       \"u_decoder_block\": DenseDecoder(**decoder_config),\n",
    "                       \"F_encoder_block\": DenseEncoder(**encoder_config),\n",
    "                       \"F_decoder_block\": DenseDecoder(**decoder_config),\n",
    "                       \"latent_config\": latent_config}\n",
    "\n",
    "## Step 3. Train 20 initial models, autoencoders-only then full model\n",
    "# create a variety of different models with randomized learning rates\n",
    "num_init_models = 3\n",
    "models = []\n",
    "\n",
    "# Set the loss functions\n",
    "loss_fns = 4*[NMSE()]\n",
    "\n",
    "# Set up validation data for autoencoders-only\n",
    "val_zeros = np.zeros(data_val_u.shape)\n",
    "val_data = [(data_val_u, data_val_f), \n",
    "            (data_val_u, data_val_f, val_zeros, val_zeros)]\n",
    "\n",
    "# Compute number of epochs to train\n",
    "aec_epochs = int(aec_only_time*60*2)  # about 2 epochs/sec\n",
    "full_epochs = int(full_model_time*60)  # about 1 epoch/sec\n",
    "\n",
    "aec_epochs = 1\n",
    "full_epochs = 3\n",
    "\n",
    "# For loop for generating the different models\n",
    "for i in range(num_init_models):\n",
    "    # Randomly selected learning rate\n",
    "    lr = 10**(-r.uniform(3, 6))\n",
    "    \n",
    "    # Create a model, initially only to train autoencoders!\n",
    "    model = AbstractArchitecture(**architecture_config,\n",
    "                                 train_autoencoders_only=True)\n",
    "    # Compile the model\n",
    "    model.compile(loss=loss_fns,\n",
    "                  optimizer=optimizer(learning_rate=lr, **optimizer_opts))\n",
    "\n",
    "    # Fit the model\n",
    "    train_zeros = np.zeros(data_train_u.shape)\n",
    "    aec_hist = model.fit(x=[data_train_u, data_train_f],\n",
    "                         y=[data_train_u, data_train_f, train_zeros, train_zeros],\n",
    "                         validation_data=val_data,\n",
    "                         callbacks=cbs,\n",
    "                         batch_size=batch_size,\n",
    "                         epochs=aec_epochs)\n",
    "    \n",
    "    # Now set the model to train all aspects (including operator L)\n",
    "    model.train_autoencoders_only=False\n",
    "    \n",
    "    hist = model.fit(x=[data_train_u, data_train_f],\n",
    "                     y=[data_train_u, data_train_f, data_train_f, data_train_u],\n",
    "                     validation_data=val_data,\n",
    "                     callbacks=cbs,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=full_epochs)\n",
    "    \n",
    "    # Append the results to the model list\n",
    "    models.append((model, hist, lr, aec_hist))\n",
    "\n",
    "\n",
    "## Step 4. Select the best model from the 20 autoencoder-only results\n",
    "\n",
    "# List of learning rates and final losses, losses averaged over final 5 epochs\n",
    "lrs = []\n",
    "final_losses = []\n",
    "\n",
    "for i in range(num_init_models):\n",
    "    _, hist, lr, _ = models[i]\n",
    "    final_losses.append(np.mean(hist.history['loss'][-5:]))\n",
    "    lrs.append(lr)\n",
    "\n",
    "\n",
    "# Select the best model, based on the minimum in the final losses\n",
    "best_model_idc = np.argmin(final_losses)\n",
    "best_model = models[best_model_idc][0]\n",
    "best_lr = lrs[best_model_idc]\n",
    "\n",
    "# Save weights for the best model\n",
    "model_weight_path = \"./data/{}_best_aec_model_weights.tf\".format(expt_name)\n",
    "best_model.save_weights(model_weight_path)\n",
    "\n",
    "\n",
    "## Optional Step: Plot learning rates vs autoencoder-only losses\n",
    "#plt.figure()\n",
    "#plt.loglog(lrs, final_losses, 'o')\n",
    "#plt.ylabel(\"Final Loss\")\n",
    "#plt.xlabel(\"Adam Learning Rate\")\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "## Step 5. Set up the full architecture run!!\n",
    "\n",
    "# Set up validation data, loss functions, and number of epochs\n",
    "val_data = [(data_val_u, data_val_f), \n",
    "            (data_val_u, data_val_f, data_val_f, data_val_u)]\n",
    "loss_fns = 4*[NMSE()]\n",
    "\n",
    "# Compute number of epochs to fit full model\n",
    "# about 1 epoch/sec in this step\n",
    "final_epochs = int(final_model_train_hrs*60*60) # 1 epoch/sec\n",
    "final_epochs = 3\n",
    "\n",
    "# Instantiate the new model\n",
    "full_model = AbstractArchitecture(**architecture_config,\n",
    "                                  train_autoencoders_only=False)\n",
    "\n",
    "# Load the weights\n",
    "full_model.load_weights(model_weight_path)\n",
    "full_model.compile(loss=loss_fns,\n",
    "                   optimizer=optimizer(learning_rate=best_lr))\n",
    "\n",
    "# Continue training the (now full) model\n",
    "hist = full_model.fit(x=[data_train_u, data_train_f],\n",
    "                      y=[data_train_u, data_train_f, data_train_f, data_train_u],\n",
    "                      validation_data=val_data,\n",
    "                      callbacks=cbs,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=full_epochs)\n",
    "\n",
    "# And save the final results!\n",
    "full_model_weights_path = \"./data/{}_final_model_weights.tf\".format(expt_name)\n",
    "full_model.save_weights(full_model_weights_path)\n",
    "\n",
    "# Look at learning rates vs AEC-only and full-model losses\n",
    "lrs = []\n",
    "full_losses = []\n",
    "aec_losses = []\n",
    "for i in range(num_init_models):\n",
    "    _, full_hist, lr, aec_hist = models[i]\n",
    "    full_losses.append(np.mean(full_hist.history['loss'][-3:]))\n",
    "    aec_losses.append(np.mean(aec_hist.history['loss'][-3:]))\n",
    "    lrs.append(lr)\n",
    "    \n",
    "## Optional Step: Plot learning rates vs autoencoder-only losses\n",
    "#plt.figure()\n",
    "#plt.loglog(lrs, full_losses, 'o', label=\"Full Models\")\n",
    "#plt.loglog(lrs, aec_losses, 'o', label=\"AEC Only\")\n",
    "#plt.ylabel(\"Final Loss\")\n",
    "#plt.xlabel(\"Adam Learning Rate\")\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "#min(aec_losses), min(full_losses)\n",
    "\n",
    "#plot_model_prediction(model, 144, data_val_u, data_val_f)\n",
    "\n",
    "## Doubled down on JSON for saving the data, since it is a uniform format!!\n",
    "\n",
    "# Get the dictionary containing each metric and the loss for each epoch\n",
    "history_dict = hist.history.copy()\n",
    "\n",
    "for key in history_dict:\n",
    "    history_dict[key] = [val.astype(np.float64) for val in history_dict[key]]\n",
    "    \n",
    "# And now dump it\n",
    "hist_filepath = \"./data/{}_model_history.json\".format(expt_name)\n",
    "json.dump(history_dict, open(hist_filepath, 'w'))\n",
    "\n",
    "# Also dump the full_losses, aec_losses, and learning rates\n",
    "initial_training = {'aec_only_loss': aec_losses,\n",
    "                    'full_init_loss': full_losses,\n",
    "                    'learn_rates': lrs}\n",
    "init_train_filepath = \"./data/{}_initial_train.json\".format(expt_name)\n",
    "json.dump(initial_training, open(init_train_filepath, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
