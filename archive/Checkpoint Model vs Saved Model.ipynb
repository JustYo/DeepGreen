{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r\n",
    "import json\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from AbstractArchitecture_v2 import AbstractArchitecture\n",
    "from DenseEncoder import DenseEncoder\n",
    "from DenseDecoder import DenseDecoder\n",
    "from NormalizedMeanSquaredError import NormalizedMeanSquaredError as NMSE\n",
    "from plot_model_prediction import plot_model_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data file locations\n",
    "data_file_prefix = './data/NLSL_expt1'\n",
    "\n",
    "# Step 1. Load in the data\n",
    "data_train_u = np.load(\"{}_train1_u.npy\".format(data_file_prefix)).astype(np.float32)\n",
    "data_train_f = np.load(\"{}_train1_f.npy\".format(data_file_prefix)).astype(np.float32)\n",
    "data_val_u = np.load(\"{}_val_u.npy\".format(data_file_prefix)).astype(np.float32)\n",
    "data_val_f = np.load(\"{}_val_f.npy\".format(data_file_prefix)).astype(np.float32)\n",
    "data_test_u1 = np.load(\"{}_test1_u.npy\".format(data_file_prefix)).astype(np.float32)\n",
    "data_test_f1 = np.load(\"{}_test1_f.npy\".format(data_file_prefix)).astype(np.float32)\n",
    "data_test_u = np.load(\"{}_test2_u.npy\".format(data_file_prefix)).astype(np.float32)\n",
    "data_test_f = np.load(\"{}_test2_f.npy\".format(data_file_prefix)).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \n",
    "\n",
    "# Load the best model\n",
    "full_model = keras.models.load_model(best_model_path, \n",
    "                                     custom_objects={\"NormalizedMeanSquaredError\": NMSE})\n",
    "\n",
    "# Continue training the (now full) model\n",
    "checkpoint_model_path = './model_weights/{}_checkpoint_full'.format(expt_name)\n",
    "cbs = [keras.callbacks.ModelCheckpoint(checkpoint_model_path,\n",
    "                                       save_weights_only=True,\n",
    "                                       monitor='val_loss',\n",
    "                                       save_best_only=True)]\n",
    "hist = full_model.fit(x=[data_train_u, data_train_f],\n",
    "                      y=[data_train_u, data_train_f, data_train_f, data_train_u],\n",
    "                      validation_data=val_data,\n",
    "                      callbacks=cbs,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=final_epochs)\n",
    "\n",
    "# Load weights with best validation loss\n",
    "full_model.load_weights(checkpoint_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs to score model\n",
    "full_model_path = \n",
    "model_weight_path = './model_weights/NLSL_Experiment_04c_checkpoint.tf'\n",
    "model_weight_path = './model_weights/NLSL_Experiment_04c_best_aec_model_weights.tf'\n",
    "#model_weight_path = './model_weights/NLSL_Experiment_03c_final_model_weights.tf'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct network:\n",
    "activation = \"relu\"\n",
    "initializer = tf.keras.initializers.VarianceScaling()\n",
    "reg_lambda_l2 = 1e-6\n",
    "regularizer = tf.keras.regularizers.l2(reg_lambda_l2)\n",
    "\n",
    "act_layer = dict(activation=activation,\n",
    "                 kernel_initializer=initializer,\n",
    "                 kernel_regularizer=regularizer)\n",
    "lin_layer = dict(activation=None,\n",
    "                 kernel_initializer=initializer,\n",
    "                 kernel_regularizer=regularizer)\n",
    "latent_config = dict(activation=None,\n",
    "                     kernel_regularizer=regularizer,\n",
    "                     use_bias=False)\n",
    "\n",
    "encoder_layers = 5\n",
    "decoder_layers = 5\n",
    "add_identity = True\n",
    "\n",
    "# Model training setting\n",
    "## Set optimizer\n",
    "optimizer = keras.optimizers.Adam\n",
    "optimizer_opts = {}\n",
    "\n",
    "# Callback function(s) and fit method options\n",
    "cbs = []\n",
    "\n",
    "############################################\n",
    "### Everything below here is automated!! ###\n",
    "############################################\n",
    "\n",
    "\n",
    "# Step 2. Set up the model architecture\n",
    "_, n = data_train_u.shape\n",
    "\n",
    "encoder_config = {'units_full': n,\n",
    "                  'num_layers': encoder_layers,\n",
    "                  'actlay_config': act_layer,\n",
    "                  'linlay_config': lin_layer,\n",
    "                  'add_init_fin': add_identity}\n",
    "\n",
    "decoder_config = {'units_full': n,\n",
    "                  'num_layers': decoder_layers,\n",
    "                  'actlay_config': act_layer,\n",
    "                  'linlay_config': lin_layer,\n",
    "                  'add_init_fin': add_identity}\n",
    "\n",
    "# Aggregate settings for model architecture\n",
    "architecture_config = {'units_latent': l,\n",
    "                       'units_full': n,\n",
    "                       'u_encoder_block': DenseEncoder(**encoder_config),\n",
    "                       'u_decoder_block': DenseDecoder(**decoder_config),\n",
    "                       'F_encoder_block': DenseEncoder(**encoder_config),\n",
    "                       'F_decoder_block': DenseDecoder(**decoder_config),\n",
    "                       'latent_config': latent_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "# Step 5. Set up the full architecture run!!\n",
    "###############################################\n",
    "\n",
    "# Set up validation data, loss functions, and number of epochs\n",
    "val_data = [(data_val_u, data_val_f), \n",
    "            (data_val_u, data_val_f, data_val_f, data_val_u)]\n",
    "loss_fns = 4*[NMSE()]\n",
    "\n",
    "# Instantiate the new model\n",
    "full_model = AbstractArchitecture(**architecture_config,\n",
    "                                  train_autoencoders_only=False)\n",
    "\n",
    "# Load the weights\n",
    "full_model.load_weights(model_weight_path)\n",
    "full_model.compile(loss=loss_fns, optimizer=optimizer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(128, 128) dtype=float32, numpy=\n",
       "array([[ 1.4649461e-01,  1.3488597e+00,  3.4818617e-03, ...,\n",
       "        -9.9592768e-02, -6.0798202e-02, -2.3643263e-01],\n",
       "       [-1.7563622e-01,  4.6216230e+00, -4.3059718e-02, ...,\n",
       "        -2.5134576e-02, -5.5155540e-01,  7.0619333e-01],\n",
       "       [-8.2934842e-02, -1.5751172e+00,  6.3790724e-02, ...,\n",
       "         3.4261156e-02,  5.2623677e-01, -2.6379162e-01],\n",
       "       ...,\n",
       "       [ 1.0836387e-01, -1.5195245e-01, -2.9770922e-02, ...,\n",
       "         1.1947747e-01,  1.3522559e+00, -4.4808277e-01],\n",
       "       [-3.4878928e-02, -9.2090733e-02,  5.6633491e-02, ...,\n",
       "         1.0114634e-01,  1.9662423e+00,  3.7870198e-01],\n",
       "       [ 9.8269403e-02, -4.9913174e-01,  3.4579262e-02, ...,\n",
       "        -1.0193128e-01, -1.4418639e+00,  2.5331366e+00]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.F_Expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128, 128), dtype=float32, numpy=\n",
       "array([[11.193986  ,  2.7592251 , -1.6591475 , ...,  0.12239839,\n",
       "        -0.07826142, -0.23726544],\n",
       "       [ 2.7592251 ,  7.138012  , -1.7727076 , ...,  0.151703  ,\n",
       "        -0.16288912, -0.03837639],\n",
       "       [-1.6591475 , -1.7727076 ,  6.351153  , ..., -0.3096179 ,\n",
       "         0.08145825, -0.42786384],\n",
       "       ...,\n",
       "       [ 0.12239839,  0.151703  , -0.3096179 , ...,  3.5000656 ,\n",
       "         0.4984079 , -0.6058025 ],\n",
       "       [-0.07826142, -0.16288912,  0.08145825, ...,  0.4984079 ,\n",
       "         3.4857    , -0.6894373 ],\n",
       "       [-0.23726544, -0.03837639, -0.42786384, ..., -0.6058025 ,\n",
       "        -0.6894373 ,  5.070664  ]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.Operator.get_operator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2. Compute scores for training, validation, and testing datasets\n",
    "#train_score = full_model.test_on_batch(x=[data_train_u, data_train_f],\n",
    "#                                       y=[data_train_u, data_train_f, data_train_f, data_train_u])\n",
    "#print(\"Training Scores:\", train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_score = model.test_on_batch(x=[data_val_u, data_val_f],\n",
    "#                                y=[data_val_u, data_val_f, data_val_f, data_val_u])\n",
    "#print(\"Validation Scores:\", val_score)\n",
    "#\n",
    "#test_score_1 = model.test_on_batch(x=[data_test_u, data_test_f],\n",
    "#                                   y=[data_test_u, data_test_f, data_test_f, data_test_u])\n",
    "#print(\"Test 1 Scores:\", test_score_1)\n",
    "#\n",
    "#test_score_2 = model.test_on_batch(x=[data_test_u1, data_test_f1],\n",
    "#                                   y=[data_test_u1, data_test_f1, data_test_f1, data_test_u1])\n",
    "#print(\"Test 2 Scores:\", test_score_2)\n",
    "#    \n",
    "#    #return train_score, val_score, test_score_1, test_score_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores = score_model(full_model, data_file_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ys = full_model.predict(x=[data_train_u, data_train_f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 1:  0.670635939\n",
      "Score 2:  0.871644378\n",
      "Score 3:  1.00046885\n",
      "Score 4:  0.603007197\n",
      "(8906, 128)\n",
      "Total Loss: tf.Tensor(3.1457562, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "true_ys=[data_train_u, data_train_f, data_train_f, data_train_u]\n",
    "nmse = NMSE()\n",
    "\n",
    "total_loss = 0\n",
    "for i, (pred_y, true_y, loss_fn) in enumerate(zip(predicted_ys, true_ys, loss_fns)):\n",
    "    #print(pred_y.dtype, true_y.dtype)\n",
    "    print(\"Score {}:\".format(i+1), end=\"  \")\n",
    "    loss = loss_fn(pred_y, true_y)\n",
    "    tf.print(loss)\n",
    "    total_loss += loss\n",
    "    \n",
    "print(data_train_u.shape)\n",
    "print(\"Total Loss:\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_model.test_on_batch(x=[data_train_u, data_train_f],\n",
    "#                         y=[data_train_u, data_train_f, data_train_f, data_train_u])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 0:  0.000135408714\n",
      "Score 1:  0.00024330795\n",
      "Score 2:  0.0406286456\n",
      "Score 3:  0.0626623631\n",
      "(8906, 128)\n",
      "Total Loss: tf.Tensor(0.103669725, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "val_x = [data_val_u, data_val_f]\n",
    "val_ys = [data_val_u, data_val_f, data_val_f, data_val_u]\n",
    "predicted_ys = full_model.predict(x=val_x)\n",
    "\n",
    "total_loss = 0\n",
    "for i, (pred_y, true_y, loss_fn) in enumerate(zip(predicted_ys, val_ys, loss_fns)):\n",
    "    #print(pred_y.dtype, true_y.dtype)\n",
    "    print(\"Score {}:\".format(i), end=\"  \")\n",
    "    loss = loss_fn(pred_y, true_y)\n",
    "    tf.print(loss)\n",
    "    total_loss += loss\n",
    "    \n",
    "print(data_train_u.shape)\n",
    "print(\"Total Loss:\", total_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
